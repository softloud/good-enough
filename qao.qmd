---
title: "Questionable Analytical Observations"
subtitle: "And what to do about them"
author: 
  - name: Dr Charles T. Gray, Datapunk
    orcid: 0000-0002-9978-011X
    affiliations:
      - name: Good Enough Data & Systems Lab

bibliography: qao.bib
csl: elsevier-vancouver.csl

number-sections: true
code-fold: true
execute:
  echo: false
  message: false
  error: true

---

```{r echo=FALSE, message=FALSE}
library(gt)


```


{{< revealjs "slides/dbt-2025.html" >}}

# Ceci n'est pas un datum

Any data engineer will tell you there's a relentlessly comedic side to the industry, wherein confusion over what an observation *is* abounds, leading to unexpectedly large deviations from vision (@fig-stonehenge).

::: {#fig-stonehenge}

<iframe src="https://www.youtube.com/embed/Pyh1Va_mYWI?si=ieVcPdnxgQ83lMGJ&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

*Nigel gave me a drawing that said 18 inches. Now, whether or not he knows the difference between feet and inches is not my problem. I do what I'm told.* -- *This is Spinal Tap* (1984) [@reiner_this_1984].

:::

Those in the trenches of the **modern data stack** (toolchains for working with data at scale [@prakash_what_2023]) know this is why the community have developed principles, such as FAIR [@wilkinson_fair_2016], and open-source tools, such as DBT-core [@dbt_labs_dbt-labsdbt-core_2025] that **opinionate** FAIR.  

These engineers are also the first to tell you just how far from reality industry and research are from realising even a fraction of those aspirations [@landi__2020] [@feuerriegel_fair_2020] [@boeckhout_fair_2018]. 

Something that tools, such as DBT, are beginning to intuit is that the governance of data must be an iterative **workflow** that interoperates with the recursive nature of inquiry, not a policy checkbox at the end--such an aim is (argued in a [sister paper](singularities.qmd)) **chaotically** impossible given how development plans morph and change as questions are refined. 

We must correct for **epistemic drift** in the chaos induced by people and machines interoperating to answer questions with observations at scale. Or, despite our best efforts, we end at the *frontier psychiatry* (@fig-frontier) of data science. 

::: {#fig-frontier}

<iframe src="https://www.youtube.com/embed/qLrnkK2YEcE?si=NW9MZlQoFAU905J4&amp;controls=0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

Each scientist or engineer is skilled in their own right, but epistemic drift creates an absurdist orchestra of bad data and broken dashboards [@the_avalanches_avalanches_2009].

:::

We begin declaritively, as engineers are busy and it's good to give them a table of instructions on the second page so they can stop reading and go code something useful. This manuscript is intended as a tool for engineers to obtain rarely-granted scope from leadership to build reusable data architecture. We then explain what motivates this framework, and provide examples in R, SQL, and Python in appendices.

# FAIR Data Entity - User Access Data Test

This test defines the first living analysis development team goal, usually instantiated in an **agile** (for democratising development [@kent_beck_manifesto_2001]) tool, such as an **epic** in JIRA [@atlassian_jira_2025]. This test draws from the V-model of systems development lifecycle that privileges end-user access [@forsberg_717_1998]. 

## First Team Goal: Instantiate the FAIR data entity test

We consider the FAIR data entity test instantiated when:

1. the data **analyst** validates minimum assumptions about a single data point in the analyst's chosen tool,
2. notably, where there is no requirement the validation passes the test.

## Framing FAIR 

The FAIR framing shown in @fig-fair is a way to communicate the black box of data to decision makers. In particular, this framing highlights the distinction between what they conceptualise as `data` (*Can't you just email me a spreadsheet?*) and a `data product` of reusable data architecture at scale that will save untold wages, time, and consternation.

```{r echo=FALSE, message=FALSE}
#| label: fig-fair
#| fig-cap: "Validation of FAIR data entity."

source("figures-and-tables/fair-minimum-criteria/fair-minimum-criteria.R")

fair_gt
```

## Minimal assumptions to test

The minimal set of tests show in @fig-tests for 

1. uniqueness
2. missingness
3. duplicates 

allows for data platform development whilst demonstrating a good-faith roadmap to compliance for, say, the EU Comission's Environmental, Social, and Governance reporting (ESG) [@european_commission_sustainable_2025]. Ideally, the analyst provides further assumptions, and a previously calculated datum, however this is notoriously difficult for engineers to obtain in practice, thus we provide a set of defaults so that engineers can begin work. 

```{r echo=FALSE, message=FALSE}
#| label: fig-tests
#| fig-cap: "FAIR data entity tests[^1]."

source("figures-and-tables/naive-tests/naive-tests.R")

test_gt
```


It can be a challenge to communicate to leadership with little experiences of the trenches of data development the obstacles in scaling legacy data to living analyses. By setting these minimal goals, the problem solving is democratised from engineer to analyst. 

We now turn to motivating the solution we just provided.

[^1]: *Contributor acknowledgement*: Many thanks to the Data & AI team at TDCnet, Denmark, for contributions to test development and advisory discussions.

# Expectation vs reality

Humans, bless 'em, are not great at conceptualising the black  boxes they work with. A critical error we make is expecting a living analysis development lifecycle to reflect the workflow of an individual analyst producing a one-off report. Thus, living analysis development tends to have an expectation backbone (@fig-anal-beads), whether it be to produce a spreadsheet, some business intelligence  dashboard, or deploy a machine learning algorithm by bespoke tooling.

::: {#fig-anal-beads}

```{r message=FALSE, echo=FALSE}
#| fig-height: 4
#| fig-width: 12
# source("figures-and-tables/anal-beads-of-data-dev/anal-beads.R")
# anal_beads_vis

source("R/ButtonCategory.R")
AnalBeads <- ButtonDesignCategory$new(preset = "anal_beads")
DiagrammeR::grViz(AnalBeads$generateDotGraph())


```

The anal beads of data development.

:::

Despite our best-laid agile plans, however, there's an inevitable drift that occurs during the predictably unpredictable recursiveness of data development (@fig-hairy-anal-beads). 


::: {#fig-hairy-anal-beads}

```{r message=FALSE}
#| fig-height: 8
#| fig-width: 12

HairyAnalBeads <- ButtonDesignCategory$new(preset = "hairy_anal_beads")
DiagrammeR::grViz(HairyAnalBeads$generateDotGraph())


```

The tangled anal beads of data development.

:::

# Democratising data development

::: {#fig-fair-entity}

```{r message=FALSE, echo=FALSE}
#| fig-height: 12
#| fig-width: 12

CareBears <- ButtonDesignCategory$new(preset = "care_bears")
DiagrammeR::grViz(CareBears$generateDotGraph())

```

Establishing a datum via FAIR data entity test allows the team to scope for specific challenges to do with different aspects of the stack. 

:::


Problem is, if you show the engineers FAIR criteria [@wilkinson_fair_2016], you will hear, *sure, sure, next week*.

> "I've found it best not to do any documetation at all, otherwise people point out what's missing." -- Real things i hear engineers say.  

The reality is while data analytic question is reworked, there is little data engineers can know are true about the data. Schema are often non-existent. Because the 

# Datum confusion -- placeholder title

# Learnings -- placeholder title

> FAIR data entity tests on some projects. Matt, I thought we could track some projects do some vis here?

# Appendices

## FAIR data entity tests for this singularity

In all tests, we ask, what is the minimum thing we can do? Then structure exists and we can expand.

### Generator fn $\to$ display

*Testing the morphism between graph visualisation generator and display.*

> How do I call code reusably across both this manuscript, a .qmd file, and a slide deck I'm embedding the same .qmd file?


```{r}
#| echo: true
#| code-fold: false

# test where I am
getwd()

# need to source
source("R/test-mooncake.R")

# test calling from R
mooncake_test

```





# References