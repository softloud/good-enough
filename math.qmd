---
title: The Math Bit
subtitle: A categorical framework for structured intelligence governance of emergence in structured intelligence systems
author: 
  - name: Dr Charles T. Gray, Datapunk
    orcid: 0000-0002-9978-011X
    affiliations:
      - name: Good Enough Data & Systems Lab
date: '`r Sys.Date()`'
bibliography: singularities.bib
number-sections: true
html-math-method: mathml

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo=FALSE,
  message=FALSE,
  error=TRUE
)

library(tidyverse)
library(ggraph)
library(tidygraph)
library(ggthemes)
library(latex2exp)


```

```{r theme}

# Define custom color palette
edge_emergence_palette <- c(
  "violation" = "#722f37",  # wine
  "miscreant" = "#cc7722",  # ochre
  "virtuous" = "#808000",   # olive green
  "expected" = "#003153"    # prussian blue
)

button_theme <- list(
  theme_tufte(),  
  theme(
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    legend.position = "top",
    legend.direction = "vertical",
    plot.title = element_text(family = "courier"),
    plot.caption = element_text(family = "courier")
  ),
  labs(
    x = "",
    y = ""
  ),
  # scale_colour_brewer(palette = "Dark2"),
  # scale_colour_manual(palette = edge_emergence_palette),
  # scale_fill_manual(values = edge_emergence_palette),
  NULL
)

button_graph <- list(
  # expects graph
  geom_edge_link(
    aes(
      label = edge_label,
      colour = edge_emergence),
    alpha=0.7,
    arrow = arrow(),
    label_alpha = 0.7
    ),
  geom_node_point(
    aes(
      shape = node_type,
      colour = node_emergence
      ),
    size = 2
  ),
  geom_node_text(
    aes(label  = node_label),
    nudge_y = -0.1,
    repel=TRUE
    )
)

button_graph_k2 <- list(
  geom_edge_arc(
    aes(
      label = edge_label,
      colour = edge_emergence),
    alpha=0.7,
    arrow = arrow(),
    label_alpha = 0.7
    ),
  geom_node_point(
    aes(shape = node_type, colour = node_emergence)
  ),
  geom_node_text(
    aes(label  = node_label),
    nudge_y = -0.1,
    repel=TRUE
    )
)


```

## Work in progress

Without ways of bounding and measuring how technology affects us, we have no chance of governing the choas of technology in our lives.
[
  Although in the experimental paper I made use of LLMs, at some point a human needs to unplug to do math. This is the typed version of what I wrote in my notebook. The writing and ideas in this document is *entirely* human emergence. 
]{.aside}

I want to know what we know, so that we know how much we don't know. Because right now I think we  all have structured intelligence governance levels in **violation** of the laws of robotics. 

Why aren't there fines for this? Maybe because no one knows how to measure it.


## Help wanted 

> *hi Tomasz, Richard, Kerrie, Sira, & friends! Thanks for taking a look.* 

Look, I'm just a data engineer who studied a skerrick of math a while back, all I wanted to do was find a way of managing the complexity of my development digraphs as the data science project progresses; also a mathematical scream at how notifications (without loss of generality) are decreasing my capacity to function as a human. 

> The only references actually used while hand writing this argument were two definitions I wrote in my notebook first.

1.  The definition of a category on wikipedia [@noauthor_category_2025].
2. A screenfreeze from a youtube video where a nice PhD student writes on a blackboard the definition of a discrete topological dynamical system. [@chalk_what_2024]

- Is it fundamentally flawed? Probably. By all means, poke holes, I'm having fun learning category theory for the first time in any case, and I miss sparring mathematically.
- Is this reinventing the wheel? I find it hard to think I would come up with something novel.

> But if it *is* feasible, why aren't we measuring the how  the instability of systems hurt humans, like, *yesterday*?! 

# Experiment: Living morphism in JIRA

Motivation for this project began with finding that despite mastering the digraph-structured planning tool JIRA; I was still utterly unable to manage the generativity nor complexity of the subgraphs, as development progresses.

All I wanted to do was construct a structure-preserving, dimension-reducing living morphism that monitored these things. What follows is a string I cannot stop pulling.

# Conjecture 

The digraph $I$ of intelligence governance is a discrete topological dynamical system that governs emergence for human wellbeing in a structured intelligence system. 

## Intuitive reasoning


Consider this system in terms of three things:

1. People.
2. Machines.
3. Relationships between people and machines.

---

<div class="tenor-gif-embed" data-postid="6132932" data-share-method="host"><a href="https://tenor.com/view/funpic-gif-6132932">Funpic GIF</a>from <a href="https://tenor.com/search/funpic-gifs">Funpic GIFs</a></div> <script type="text/javascript" async src="https://tenor.com/embed.js"></script>

---

A category-theoretic way of measuring the stabilty of the system might be to ask:

> How many of the people, machines, and relationships between are operational? 

A intelligence-governance question might be to ask:

> Which people, machines, or relationships are exhibiting an emergent behaviour that is harming the humans in the system?

## Example: Cycle of intelligence governance visualisation

We will use code examples to explain mathematical structures. 

We begin by constructing the digraph $I$ as a computational object `I_vis`, a `ggraph` visualisation  of the category.

::: {.panel-tabset}


## Edges

```{r echo=FALSE}

I_edges <- tribble(
  ~from,  ~to,  ~edge_emergence,
  "R",    "I",  "miscreant",
  "I",    "R",  "expected",
) |>
  mutate(
    edge_label = str_c(from, to),
    edge_emergence = factor(edge_emergence, 
    levels = c("miscreant", "expected"))
  )

```

```{r echo=TRUE}
I_edges
```


## Nodes


```{r error=FALSE}

I_nodes <- I_edges |>
  select(node = from)  |>
  mutate(
    node_label =   node,
    node_label = case_when(
      node == "I" ~ "Intention (I)",
      node == "R" ~ "Heuristic (R)" 
    ),
    node_type = node_label,
    node_emergence = if_else(
      node == "R", "miscreant", "expected"
    ) 
  )

```


```{r echo=TRUE}
I_nodes
```

## Graph

```{r echo=FALSE}

I_graph <- I_edges |>
  as_tbl_graph() |>
  activate(nodes) |>
  left_join(I_nodes, by = c("name" = "node"))


```


```{r echo=TRUE}
I_graph
```

## Visualisation


```{r echo=FALSE}

I_vis <- I_graph |>
  ggraph(layout="linear") +
  button_graph_k2 +
  button_theme +
  labs(
    title = "Cycle of intelligence governance"
  ) 



```


```{r echo=TRUE}
I_vis 

```

:::

```{r echo=FALSE,eval=FALSE}

# why can't I get the colour palette to work?

I_vis +
  scale_colour_manual(
    values = c(
      "miscreant" = edge_emergence_palette[["miscreant"]],
      "violation" = edge_emergence_palette[["violation"]]
    )
  )

```

## Proof framework

To show this is true,  we will need to:


1. Define a structured intelligence system.
2. Define the category $\mathcal G$ of structured intelligence governance.
3. Define $I$ as a digraph in terms of the category of structured intelligence governance.
4. Define constraints on the morphisms of $\mathcal G$ in terms of $I$.
5. Show $I$ is a discrete topological dynamical system.
6. Show how $I$ governs emergence in a structured intelligence system.

# Structured intelligence systems

## Definition: Structured intelligence system

A **structured intelligence system** $S$ is a directed graph $S = (H \cup A, M)$ representing humans and automata interoperating toward some intention with objects and morphisms (unrigorously  inspired by this paper I do not have access to [@badreddin_structured_2006]).

- Nodes $H \cup A$ are understood as collection of humans $h \in H$ that interoperate with automata $a \in A$ toward a human-defined intention.
- Edge-relations $M$ representing the binary interoperations of the elements in the system:
  - $hh$: Humans talk, reflect, interact.
  - $ha$: Human instantiates heuristics on automata with intention.
  - $aa$: Automata stack without human intervention.
  - $ah$: Automaton outputs results of heuristics applied to input.

## Example: Structured intelligence system

::: {.panel-tabset}

## Humans and automata

```{r echo=FALSE}
humans <- paste("h", seq(1:3), sep="_")

```

```{r echo=TRUE}
humans
```

```{r echo=FALSE}
automata <- paste("a", seq(1:2), sep="_")

```

```{r echo=TRUE}
automata
```

## Edges

```{r echo=FALSE}


S_edges <- tribble(
  ~from,  ~to,   ~edge_label,
  "h_11",  "a_11", "writes report",  
  "h_11",  "a_21", "shares data",
  "h_11",   "h_2",  "describes data",  
  "a_11",  "h_12", "compiles document",
  "a_21",  "h_2", "receives data",
  "h_2",  "a_22", "shares analysis",
  "a_22",  "h_12", "receives analysis",
  "h_2",  "h_12", "reports analysis",
  "h_12", "a_12",  "incorporates analysis",
  "a_12",   "h_13", "compiles report",
  "h_13", "h_3",  "reports results"
) |>
  mutate(
    edge_emergence = case_when(
      edge_label %in% c(
        "describes data", 
        "reports analysis",
        "incorporates analysis",
        "compiles report",
        "reports results"
        ) ~ "miscreant",
      TRUE ~ "expected"
    )
  )

```

```{r echo=TRUE}
S_edges

```

## Nodes

```{r echo=FALSE}

S_nodes <- S_edges |>
  select(node = from)  |>
  # ensure unique now necessary
  bind_rows(
    S_edges |> select(node = to)
  ) |>
  distinct() |>
  mutate(
    # needed to distinguish between type and node
    node_type =   
    case_when(
      str_detect(node, "h_") ~ "Human (h)",
      str_detect(node, "a_") ~ "Automaton (a)"      
    ),
    node_label = node,
    node_emergence = if_else(
      node == "h_2", "miscreant", "expected"
    ) 
  )



```

```{r echo=TRUE}
S_nodes 

```

## Graph

```{r echo=FALSE}
S_graph <- S_edges |>
  as_tbl_graph() |>
  activate(nodes) |>
  left_join(S_nodes, by = c("name" = "node")) 

```

```{r echo=TRUE}

S_graph

```

## Visualisation


```{r echo=FALSE}

S_vis <- S_graph |>
  ggraph() +
  button_graph +
  button_theme +
  labs(
    title = "Structured Intelligence System",
    caption = "A structured intelligence system describes humans and automata and their interoperations." 
  ) 



```


```{r echo=TRUE}
#| fig-height: 12

S_vis 

```
:::


## Terms in $S$

### Definition: Humans

We invoke **human** in the conventional sense of the term and note that humans self-generate intention.

### Definition: Intention

An **intention** requires human thought and is self-generating.

### Definition: Automata

An automaton $a_r : I \mapsto O$, with heuristics (rules written by humans) $r \in R$, transforms human intention $i \in I$ and  to some output $o \in O$. An automata does not have intention innately, only rules. For the purposes of this framework, we will generally think of an automataton $a$ as an object.

# Governance

So, we now have automata interoperating with people. Think of *The Matrix* [@wachowski_matrix_1999] as a metaphor for our systems of technology in which humans are overwhemed by emergences. And, as Dr Salvin Calvin rightly observed in *Robot Dreams* [@asimov_i_1950], any robot in violation of the three laws must be immediately terminated.

## Definition: The Three Laws of Robotics

As defined by Asimov in *I, Robot* [@asimov_i_1950].

1. A robot may not injure a human being or, through inaction, allow a human being to come to harm.
2. A robot must obey orders given it by human beings except where such orders would conflict with the First Law.
3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.

We must govern structured intelligence systems to ensure they do not harm humans. And we must learns from the metaphors, in particular the metonyms deconstructed in critical theory, to construct rigorous categorical frameworks that can be computationally and ethically applied to structural intelligence systems. 

<div class="tenor-gif-embed" data-postid="25954630" data-share-method="host" data-aspect-ratio="1.78771" ><a href="https://tenor.com/view/i-robot-sonny-hiding-subterfuge-gif-25954630">I Robot Sonny GIF</a>from <a href="https://tenor.com/search/i+robot-gifs">I Robot GIFs</a></div> <script type="text/javascript" async src="https://tenor.com/embed.js"></script>

## Definition: The three laws of structured intelligence governance

Adapted from [the three laws](#definition-the-three-laws-of-robotics).

1. A structured intelligence system may not injure a human being or, through inaction, allow a human being to come to harm.
2. A structured intelligence system must obey orders given it by human beings except where such orders would conflict with the First Law.
3. A structured intelligence system must protect its own existence as long as such protection does not conflict with the First or Second Law.

This leads us to the question, how might we detect if a system is in violation of the laws?

## Emergence in development on the modern data stack

Without loss of generality, consider a large-scale project on the modern data stack involving many people and computational tools they use.

Any developer, or any person on a team for that matter, will tell you that at kick off, leadership are convinced that the plan is well defined. However, any developer will also tell you they've never seen a well-defined plan. Every developer is living in a singularity of tasks that are required to be computationally isolated, perhaps change a number from `3`to `4` i n particular file. All of these  tasks are meant to reassemble into leadership's vision. Despite the advances of agile, chaos still reigns. Development tasks are dinosaurs in the Jurassic Park of structured intelligence systems.  

<div class="tenor-gif-embed" data-postid="15354606320925778627" data-share-method="host" data-aspect-ratio="1.77857"><a href="https://tenor.com/view/raptors-what-gif-15354606320925778627">Raptors What GIF</a>from <a href="https://tenor.com/search/raptors+what-gifs">Raptors What GIFs</a></div> <script type="text/javascript" async src="https://tenor.com/embed.js"></script>

The result of ungoverned structured intelligence systems? Developers are blamed and traumatised by unfair demands; the wellbeing of humans in the system they interoperate within, the singularity, is not considered. This is shredding the very talent we need to solve critical problems facing humanity, such as climate change and inequality.

## Definition: Emergence on a structured intelligence system

::: {#fig-emergence-order}

```{dot}
graph E {
  violation -- miscreant
  violation -- virtuous
  miscreant -- expected
  virtuous -- expected
}

```

Hasse diagram [@davey_homomorphism_2018] of emergence ordered by severity.

:::


We consider the elements preimage of $\mathcal G$ to display **emergence** $\mathbf e$ in one of four ways:
  
  - Expected.
  - Violation: A violation of one the [three laws].(#definition-the-three-laws-of-structured-intelligence-governance)
  - Miscreant: breaks commutativity in $\mathcal G$, increases length of a path in structured intelligence system, or increases periodicity in $I$.
  - Virtuous: decreases length of a path in *any* structured intelligence system, decreases periodicity in $I$, or produces unexpected but useful results.

So that

$$
e : \mathtt{ob}(\mathcal G) \cup \mathtt{hom}(\mathcal G) \mapsto \{
  \mathtt{expected},
  \mathtt{violation},
  \mathtt{miscreant},
  \mathtt{virtuous}
\}
$$

under the order shown in Figure @fig-emergence-order.


# Category of structured intelligence governance

## Definition: Category of structured intelligence governance

The category [@noauthor_category_2025] $\mathcal G$ of structured intelligence governance is isomorphic to $K_2$ cycle with loops comprising:

- Objects $\mathtt{ob}(\mathcal G) = \{H, A \}$:
  - $H$ from the humans in the nodes of $S$;
  - $A$ from the automata in the nodes of $S$.

- Morphisms $\mathtt{hom}(\mathcal G)$:
  - $HH$: humans interact or self-reflect;
  - $HA$: humans instantiate intent on automata;
  - $AA$: automata stack without human intervention;
  - $AH$: automata output with constraints

- Governance constraint measures assign one of the four levels of emergence (expected, violation, miscreant, virtuous) to both the objects and the morphisms.

## Visualisation: Category of structured intelligence governance

::: {.panel-tabset}

## Edges

```{r echo=FALSE}
G_edges <- tribble(
  ~from, ~to, ~edge_emergence,
  "H",      "A",  "expected",
  "A",      "H",  "miscreant"
) |>
  mutate(edge_label = str_c(from, to))

```


```{r echo=TRUE}
G_edges
```

## Nodes

```{r echo = FALSE}
G_nodes <- tibble(
  node = c(G_edges$from, G_edges$to)
) |> distinct() |>
  mutate(
    node_type = node,
    node_emergence = if_else(
      node == "A",
      "miscreant",
      "expected"
    ),
    node_label = node
  )


```

```{r echo = TRUE} 
G_nodes
```

## Graph

```{r echo = FALSE}
G_graph <- G_edges |>
  as_tbl_graph() |>
  activate(nodes) |>
  left_join(G_nodes, by = c("name" = "node")) 

```

```{r echo = TRUE} 
G_graph

```


## Visualisation

```{r echo=FALSE}
G_vis <- G_graph |>
  ggraph() +
  button_graph_k2 +
  button_theme +
  labs(
    title = "Category of Structured Intelligence Governance",
    caption = "A structured intelligence system describes humans and automata and their interoperations." 
  ) 

```

:::


# Cycle of intelligence governance

## Definition: Cycle of intelligence governance

The cycle of intelligence governence $I$ comprises:

- Nodes: ?
  - $\mathfrak R$, all objects in the pre-image of $\mathcal G$ that are following heuristic reasoning, such that $\mathfrak R(n, t)$ represents $\mathfrak R$ at the $n$ th step in the cycle, and $t$ denotes the time that step takes. 
  - $\mathfrak I$, all objects in the pre-image of $\mathcal G$ that are following intentional reasoning, such that $\mathfrak I(n, t)$ represents $\mathfrak I$ at the $n$ th step in the cycle, and $t$ denotes the time that step takes. 
- Edges:
  - ?

## Visualisation of cycle of intelligence governance

```{r echo=TRUE}
I_vis
```

# Singularities

## Definition: Technological singularity

A technololgical singularity is when the cycle of intelligence governance displays emergence beyond a constraint threshold for human governance. A singularity may be above a virtuous threshold, miscreant threshold, or a mix of both.

## Definition: Singularity threshold

Define the  bounds of a metric on $I_S$ that aggregates the emergence of a structured intelligence system $S$,

$$
E = \begin{cases} 
\infty \hspace{1em} \text{if there exists an object in the preimage of $\mathcal G$ in}\\
\text{violation of the laws,}\\ 
g(\alpha n + \beta t + \varepsilon_v (H, I) + \varepsilon_m (H, I)), \text{ otherwise}. 
\end{cases}
$$

where $n$ denotes the periodicity of the cycle in $I$, and $t$ denotes the time elapsed and $g$ denotes a generalised linear regression (ARIMA) model where time is an ordered pair $(n, t) \in N \times T$ of total steps $N$ and time duration of system operation $T$. The other coefficients $\varepsilon$ describe the intelligence governance  measures (in terms of specificity and sensitivity) for miscreant $\varepsilon_m$  and virtuous $\varepsilon_m$ emergence on the objects in the intelligence governance digraph.

> yes I'm aware I have a problem with using $I$ twice - sorry for the confusion! 

Since we have finite, generative digraphs that iteratively change in structure over steps in the paths of the digraph related to time (development, in say, weeks),  this suggests topological dynamics has the answers to ways to measure stability in a system such as this. 

By using an ARIMA model, we can extract emergence thought off in terms of `miscreant` or `virtuous` in ways critical theory can apply their categorical frameworks to say, metonyms [@radden_metonymy_1999], to assist mathematicians in defining bounds on emergence in ways that can be computationally instantiated.

## Definition: Unbounded singularity

An unbounded singularity has no measure by which to check the laws; an unbounded singularity is in **violation** of the laws as we cannot confirm the system is *not* in violation.

## Definition: Virtuous singularity

A singularity in which virtuous E >> miscreant E, i.e., $\varepsilon_v \gg \varepsilon_m$.

## Definition: Miscreant singularity

A singularity in which miscreant E >> virtuous E, i.e., $\varepsilon_m \gg \varepsilon_v$.

## Definition: Singularity violation

A singularity in which there exists a violation of the laws, i.e., $E = \infty$.

## Definition: Violation of the laws

1. We say a singularity has violated Law 1. of the laws fo [structured intelligence governance](#definition-the-three-laws-of-structured-intelligence-governance) when there exist a non-empty subset of $H$ with emergence severity *violation*.

2. We say a singularity has violated Law 2. when $I$ contains elements of $A$.

3. We say a singularity has violated Law 3. when there are $aa$ morphisms with unexpected output.   

# Conjecture: The modern data stack is in violation of the three laws

> Harmful emergence is [unbounded](#definition-unbounded-singularity) in development existence proof.

# Conjecture: The social media industry is in violation of the three laws

> Harmful emergence is [unbounded](#definition-unbounded-singularity) in social media apps existence proof. (e.g., FB in Myanmar)


# References